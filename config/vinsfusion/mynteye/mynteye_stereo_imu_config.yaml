%YAML:1.0

#common parameters
#support: 1 imu 1 cam; 1 imu 2 cam: 2 cam;
imu: 1
num_of_cam: 2

imu_topic: "/mynteye/imu/data_raw"
image0_topic: "/mynteye/left/image_raw"
image1_topic: "/mynteye/right/image_raw"
output_path: "/Bulk_Data/vins-mono-output/"

#cam0_calib: "camera_left_pinhole.yaml"
#cam1_calib: "camera_right_pinhole.yaml"
cam0_calib: "camera_left.yaml"
cam1_calib: "camera_right.yaml"
image_width: 752
image_height: 480

# contains the right_T_left aka 1_T_0, ie. stereo baseline. I need right_T_left that a transform that transforms 3d points of left to 3d points of right. in other words, if you sure of right the position of left camera.
# This is an optional argument even if using `num_of_cam:=2`. If I cannot
# find this key I will use `body_T_cam0` and `body_T_cam1` to compute cam1_T_cam0.
# But if this key exists, we will use these values as stereo baseline.
# **In this file, I assume translation re specified ****in mm**** (and not in meters).**
extrinsic_1_T_0: "extrinsics.yaml"

# Extrinsic parameter between IMU and Camera.
estimate_extrinsic: 0   # 0  Have an accurate extrinsic parameters. We will trust the following imu^R_cam, imu^T_cam, don't change it.
                        # 1  Have an initial guess about extrinsic parameters. We will optimize around your initial guess.

body_T_cam0: !!opencv-matrix
   rows: 4
   cols: 4
   dt: d
   data: [4.2739206953025244e-03, -9.9997227378122833e-01,-6.0979726705474944e-03,6.1040452082857912e-03,
            9.9998919890819160e-01, 4.2626966744552242e-03, 1.8524265207998580e-03,-4.8408978130397302e-02,
            -1.8263813521932205e-03, -6.1058239298286089e-03, 9.9997969141642784e-01,2.2594515206886469e-02,
            0,0,0,1]

#  ===> imu_T_cam0 * inv( cam1_T_cam0 )
body_T_cam1: !!opencv-matrix
   rows: 4
   cols: 4
   dt: d
   data: [ 0.00174443, -0.99998733, -0.0046694 ,  0.00665227,
        0.99998578,  0.00172106,  0.00500326,  0.07165957,
       -0.00499517, -0.00467806,  0.9999766 ,  0.02216417,
        0.        ,  0.        ,  0.        ,  1.        ]

#Multiple thread support
multiple_thread: 1

#feature traker paprameters
max_cnt: 120            # max feature number in feature tracking
min_dist: 20            # min distance between two features
freq: 10                # frequence (Hz) of publish tracking result. At least 10Hz for good estimation. If set 0, the frequence will be same as raw image
F_threshold: 1.1        # ransac threshold (pixel)
show_track: 0           # publish tracking image as topic
flow_back: 0            # perform forward and backward optical flow to improve feature tracking accuracy

#optimization parameters
max_solver_time: 0.05  # max solver itration time (ms), to guarantee real time
max_num_iterations: 12   # max solver itrations, to guarantee real time
keyframe_parallax: 10.0 # keyframe selection threshold (pixel)

#imu parameters       The more accurate parameters you provide, the better performance
acc_n: 0.04          # accelerometer measurement noise standard deviation. #0.2   0.04
gyr_n: 0.004         # gyroscope measurement noise standard deviation.     #0.05  0.004
acc_w: 0.0004         # accelerometer bias random work noise standard deviation.  #0.02
gyr_w: 2.0e-5       # gyroscope bias random work noise standard deviation.     #4.0e-5
g_norm: 9.80766     # gravity magnitude

#unsynchronization parameters
estimate_td: 0                      # online estimate time offset between camera and imu
td: 0.0                             # initial value of time offset. unit: s. readed image clock + td = real image clock (IMU clock)

#loop closure parameters
load_previous_pose_graph: 0        # load and reuse previous pose graph; load from 'pose_graph_save_path'
pose_graph_save_path: "~/output/pose_graph/" # save and load path
save_image: 0                   # save image in pose graph for visualization prupose; you can close this function by setting 0
